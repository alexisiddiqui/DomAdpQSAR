{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notebook for Domain Adaptation in QSAR\n",
    "# Based on data from FLuid Notebook\n",
    "# Need to find a way to pull the datasets from this and use them here\n",
    "\n",
    "\n",
    "import torch\n",
    "import os\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = 'mps'\n",
    "# get this to work using mps\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import Draw, AllChem\n",
    "IPythonConsole.ipython_useSVG=True \n",
    "Chem.MolFromSmiles(\"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Center output\n",
    "#from IPython.display import display, HTML\n",
    "CSS = \"\"\"\n",
    ".output {\n",
    "    align-items: center;\n",
    "}\n",
    "\"\"\"\n",
    "#HTML('<style>{}</style>'.format(CSS))\n",
    "\n",
    "# Enables large output display\n",
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>div.output_scroll { height: 44em; }</style>\"))\n",
    "\n",
    "#from google.colab import data_table\n",
    "#data_table.enable_dataframe_formatter()\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show(obj):\n",
    "  display(HTML(obj.to_html(escape=False)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to load in the data from the fluid notebook\n",
    "# TODO setup script to generate and then pull data from fluid notebook\n",
    "# then load data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import FLuID as fluid\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# import parameters ### TODO actually extract parameters to a separate file\n",
    "\n",
    "import parameters.base_parameters as parameters\n",
    "\n",
    "print(parameters.lhasa_params)\n",
    "\n",
    "\n",
    "k = 8\n",
    "\n",
    "params = {\n",
    "    \n",
    "    # experiment details\n",
    "    'details' : 3,                  # level of detail of the experiment (low=1,medium=2,high=3,full=4)\n",
    "    \n",
    "    # datafiles\n",
    "'training_data_file' : 'hERG_lhasa_training',\n",
    "    'test_data_file' : 'hERG_lhasa_test',\n",
    "'transfer_data_file' : 'FLuID_full',\n",
    "  'fluid_label_file' : 'FLuID_labels',\n",
    "    \n",
    "    # data sampling\n",
    "   'validation_ratio': 0.2,         # ratio validation/training\n",
    "     'transfer_size' : 50000 ,      # sample for the transfer data (-1 = all)\n",
    "         'test_size' : -1,          # sample for the test data (-1 = all)\n",
    "     'training_size' : -1,          # sample for the training data (-1 = all)\n",
    "\n",
    "    # number of teacher/clusters (kMean)\n",
    "                 'k' : k,           # number of clusters (kMean)\n",
    "     'smooth_factor' : 0.05,        # level of post-clustering mixing to avoid fully biased teachers\n",
    "    \n",
    "    # teachers\n",
    " 'teacher_algorithm' : 'rf',        # algorithm used to build the teacher models\n",
    "    \n",
    "    # students\n",
    " 'federated_student' : 'F' + str(k),\n",
    "      'student_size' : 10000,                                              # size of the student (number of labelled Cronos data used)\n",
    "      'student_sizes' : [100,250,500, 1000,2500,5000,10000,25000,50000],   # sizes of the student ti study the impact of the size\n",
    " 'student_algorithm' : 'rf',                                               # default algorithm used to build the student models\n",
    "      'student_mode' : 'balanced',                                         # default mode used to select the student data \n",
    "    \n",
    "    # random seed for reproductibility\n",
    "      'random_state' : 42,\n",
    "\n",
    "    # t-SNE settings\n",
    "         'tsne_size' : 500,\n",
    "   'tsne_iterations' : 1000,\n",
    "    \n",
    "    # replication level\n",
    "    'replicate_count' : 3,\n",
    "    \n",
    "    # fonts\n",
    "       'figure_font' : dict(family=\"Arial\",size=14,color=\"black\"),\n",
    " 'small_figure_font' : dict(family=\"Arial\",size=10,color=\"black\"),\n",
    "\n",
    "    # colors\n",
    "'figure_color_scale' : [(0,\"red\"),(0.2,\"orange\"), (0.3,'yellow'),(1,'green')],\n",
    "        'bar_colors' : px.colors.qualitative.Prism,\n",
    "         'green_map' : plt.get_cmap('Greens')\n",
    "}\n",
    "\n",
    "base_params = params.copy()\n",
    "\n",
    "\n",
    "base_params[\"FP_type\"] = \"ECFP4\"\n",
    "base_params[\"FP_radius\"] = 2\n",
    "base_params[\"FP_length\"] = 2**11\n",
    "\n",
    "\n",
    "base_params[\"regressor_layers\"] = [base_params[\"FP_length\"], \n",
    "                                   base_params[\"FP_length\"], \n",
    "                                   base_params[\"FP_length\"]//2**2, \n",
    "                                   base_params[\"FP_length\"]//2**4, \n",
    "                                   1] # slightly modified from the paper to use powers of 2 for convenience\n",
    "base_params[\"regressor_dropout\"] = [0.33] # taken from paper\n",
    "base_params[\"max_epochs\"] = 100\n",
    "base_params[\"batch_size\"] = 2**7\n",
    "base_params[\"learning_rate\"] = 10**-4 ### TODO Check this is correct - find this from the paper\n",
    "base_params[\"convergence_threshold\"] = 0.01\n",
    "\n",
    "base_params[\"convergence_criterion\"] = \"\"\n",
    "\n",
    "base_params[\"base_checkpoint_dir\"] = \"model_checkpoints\"\n",
    "base_params[\"base_results_dir\"] = \"model_results\"\n",
    "os.makedirs(base_params[\"base_results_dir\"] , exist_ok=True)\n",
    "\n",
    "base_params[\"data_dir\"] = \"data\"\n",
    "\n",
    "for dataset in [\"training_data\", \"test_data\", \"transfer_data\", \"validation_data\", \"label_table\", \"federated_data\"]:\n",
    "  base_params[dataset] = os.path.join(base_params[\"data_dir\"], dataset + \".pkl\")\n",
    "\n",
    "\n",
    "FT_params = base_params.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(fluid)\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load datasets -- TODO make this a seperate script to pull from fluid notebook\n",
    "\n",
    "# Federated - load in transfer data\n",
    "federated_data = pd.read_pickle(base_params[\"federated_data\"])\n",
    "\n",
    "# Clean - load in training data\n",
    "clean_data = pd.read_pickle(base_params[\"training_data\"])\n",
    "\n",
    "# Validation - load in validation data\n",
    "validation_data = pd.read_pickle(base_params[\"validation_data\"])\n",
    "\n",
    "\n",
    "# Target - load in test data\n",
    "target_data = pd.read_pickle(base_params[\"test_data\"])\n",
    "\n",
    "\n",
    "#pre calculate fingerprints for all molecules\n",
    "\n",
    "### Currently just computed within the fluid notebook\n",
    "\n",
    "\n",
    "### split the data into training and validation sets\n",
    "\n",
    "### Currently just using the split from the fluid notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_data.CLASS.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.nn.modules.module import Module\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self, layersize=[2**11, 2**11, 2**9, 2**7, 2**0], dropout=0.33):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.hidden = nn.ModuleList()\n",
    "        self.batchnorm = nn.ModuleList()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        for idx, layer in enumerate(layersize[:-2]):\n",
    "            self.hidden.append(nn.Linear(layersize[idx], layersize[idx+1]))\n",
    "            self.batchnorm.append(nn.BatchNorm1d(layersize[idx+1]))\n",
    "\n",
    "        self.output = nn.Linear(layersize[-2], layersize[-1])  # output layer for binary classification\n",
    "\n",
    "\n",
    "        # save names for each layer\n",
    "        for idx, layer in enumerate(self.hidden):\n",
    "            self.hidden[idx].name = f\"hidden_{idx}\"\n",
    "        for idx, layer in enumerate(self.batchnorm):\n",
    "            self.batchnorm[idx].name = f\"batchnorm_{idx}\"\n",
    "        self.output.name = \"output\"\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for idx, layer in enumerate(self.hidden):\n",
    "            # print(f\"hidden layer {idx} output shape: {x.shape}\")\n",
    "            x = F.relu(self.hidden[idx](x))\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "            x = self.batchnorm[idx](x)\n",
    "\n",
    "            if idx == len(self.hidden) - 1:\n",
    "                last_hidden = x  # save activation of last hidden layer\n",
    "        # print(f\"output layer input shape: {x.shape}\")\n",
    "        # print(f\"output layer output shape: {self.output(x).shape}\")\n",
    "        output = torch.sigmoid(self.output(x))  # apply sigmoid activation to output layer for binary classification\n",
    "    \n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate model \n",
    "\n",
    "# load Classifier???\n",
    "# from models import Classifier\n",
    "\n",
    "model = Classifier()\n",
    "\n",
    "print(model.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        self.fp = self.dataframe['FP'].to_numpy()\n",
    "        self.labels = self.dataframe['CLASS'].to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = torch.tensor(self.fp[index], dtype=torch.float32, device=device)\n",
    "        y = torch.tensor(self.labels[index], dtype=torch.float32, device=device)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataframe, rank=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.fp = self.dataframe['FP'].to_numpy()\n",
    "        self.labels = self.dataframe['CLASS'].to_numpy()\n",
    "        if rank is not None:\n",
    "            self.rank = self.dataframe['RANK'].to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = torch.tensor(self.fp[index], dtype=torch.float32, device=device)\n",
    "        y = torch.tensor(self.labels[index], dtype=torch.float32, device=device)\n",
    "        \n",
    "        if hasattr(self, rank):\n",
    "            r = torch.tensor(self.rank[index], dtype=torch.float32, device=device)\n",
    "            return x, y, r\n",
    "        else: \n",
    "            return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# broad tuning on federated dataset\n",
    "FT_params[\"experiment_name\"] = \"broad_tuning\"\n",
    "FT_params[\"checkpoint_dir\"] = os.path.join(FT_params[\"base_checkpoint_dir\"], FT_params[\"experiment_name\"])\n",
    "# make directory for checkpoints\n",
    "os.makedirs(FT_params[\"checkpoint_dir\"], exist_ok=True)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# create a dataloader for the federated data\n",
    "\n",
    "N = 2**10\n",
    "\n",
    "\n",
    "federated_loader = MyDataset(federated_data)\n",
    "validation_loader = MyDataset(validation_data)\n",
    "training_loader = MyDataset(clean_data)\n",
    "testing_loader = MyDataset(target_data)\n",
    "\n",
    "N = 25000\n",
    "federated_loader = DataLoader(federated_loader, batch_size=N, shuffle=True)\n",
    "\n",
    "N = 128\n",
    "validation_loader = DataLoader(validation_loader, batch_size=N, shuffle=True)\n",
    "training_loader = DataLoader(training_loader, batch_size=N, shuffle=True)\n",
    "testing_loader = DataLoader(testing_loader, batch_size=N, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, layer in enumerate(base_params[\"regressor_layers\"]):\n",
    "    print(idx, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO make this work to update the plot during each loop to show the progress\n",
    "# may have to use interactive mode\n",
    "\n",
    "def plot_losses(losses, logscale=True, ax=None, title=None, save=False, filename=None):\n",
    "    \"\"\"\n",
    "    Plots the losses for the model\n",
    "    Inputs:\n",
    "    - losses ... dictionary of losses\n",
    "    - ax ... existing matplotlib axes object to plot on (default=None)\n",
    "    - title ... title of the plot (default=None)\n",
    "    - save ... boolean to save the plot (default=False)\n",
    "    - filename ... name of the file to save the plot as (default=None)\n",
    "    Outputs:\n",
    "    - plot of the losses\n",
    "    \"\"\"\n",
    "    if title is None:\n",
    "        title = \"Losses on current dataset\"\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "        \n",
    "\n",
    "    # check for convergence\n",
    "    conv = 0.99\n",
    "    convergence = []\n",
    "    for idx,_ in enumerate(losses[0]):\n",
    "        convergence.append(conv**(idx))\n",
    "    pre = ''\n",
    "    if logscale is True:\n",
    "        losses = np.log(losses)\n",
    "        convergence = np.log(convergence)\n",
    "        pre = 'log '\n",
    "    # plot the losses\n",
    "    ax.plot(losses[0], label='train')\n",
    "    ax.plot(losses[1], label='val')\n",
    "    ax.plot(convergence, label='exp decay '+str(conv)+'^epoch')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel(pre+'Loss')\n",
    "    # ax.set_ylim([-8, 3])\n",
    "    ax.legend()\n",
    "    if save is True and filename is not None:\n",
    "        plt.savefig(filename+'.png')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import datetime\n",
    "import glob\n",
    "def save_model(model, params=None, path=None):\n",
    "    \"\"\"Save model dict to file, use parameter dictionary to save to path, \n",
    "    if not save to current directory of the current date and time\"\"\"\n",
    "    \n",
    "    current_date_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    name = \"model\"\n",
    "    if path is not None:\n",
    "        prefix = path\n",
    "    \n",
    "    if path is None:\n",
    "        prefix = os.getcwd()\n",
    "\n",
    "    if params is not None:\n",
    "        name = params[\"experiment_name\"]\n",
    "        prefix = params[\"checkpoint_dir\"]\n",
    "\n",
    "\n",
    "    path = os.path.join(prefix, name+\"_\"+current_date_time)\n",
    "\n",
    "    path = path + '.pt'\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "    return path\n",
    "\n",
    "\n",
    "\n",
    "def load_model(model, path, latest=False):\n",
    "    \"\"\"Load model state dict from file, \n",
    "    if latest is True, load latest model from directory\"\"\"\n",
    "    if latest:\n",
    "        path = max(glob.glob(path + \"/*.pt\"), key=os.path.getctime)\n",
    "        print(f\"Loading latest model from {path}\")\n",
    "\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from torch.nn.modules.module import Module\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=1, lr=0.001, weight_decay=0.0):\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.BCELoss() # CE for classifcation\n",
    "    # criterion = nn.MSELoss() # MSE for regression\n",
    "\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Track losses and accuracies\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # clear axes for plotting later\n",
    "    ax = None\n",
    "    print(device)\n",
    "\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Train loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set model to train mode\n",
    "        model.train()\n",
    "\n",
    "        # Train on batches\n",
    "        train_loss = 0\n",
    "        for idx, data in enumerate(train_loader):\n",
    "            # print(str(epoch)+\":\"+str(idx), end=)\n",
    "            x, y = data\n",
    "            # Move data to GPU\n",
    "            # x, y = x.to(device), y.to(device)\n",
    "\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(x)\n",
    "            # reshape y to match output shape\n",
    "            y = y.reshape(outputs.shape)\n",
    "            # loss \n",
    "            loss = criterion(outputs, y)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Compute average training loss for epoch\n",
    "        # train_loss = np.mean(train_loss)\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                # Forward pass\n",
    "\n",
    "                outputs = model(x)\n",
    "                y = y.reshape(outputs.shape)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, y)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Compute average validation loss for epoch\n",
    "\n",
    "        # val_loss = np.mean(val_loss)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Print loss for epoch\n",
    "        print(f\"Epoch {epoch + 1}: Train loss = {train_loss:.4f}, Val loss = {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "        # plot_losses([train_losses, val_losses])\n",
    "\n",
    "    # Print final losses\n",
    "    print(f\"Final: Train loss = {train_loss:.4f}, Val loss = {val_loss:.4f}\")\n",
    "    # save model\n",
    "    return model, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_params[\"experiment_name\"] = \"lr_1e-3_broad\"\n",
    "base_params[\"checkpoint_dir\"] = os.path.join(base_params[\"base_checkpoint_dir\"], base_params[\"experiment_name\"])\n",
    "os.makedirs(base_params[\"checkpoint_dir\"], exist_ok=True)\n",
    "model = Classifier()\n",
    "model, train_losses, val_losses = train_model(model, federated_loader, validation_loader, num_epochs=100, lr=base_params[\"learning_rate\"])\n",
    "plot_losses([train_losses, val_losses], title=\"Losses on federated dataset\")\n",
    "save_model(model, params=base_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_params[\"weight_decay\"]=0.42\n",
    "base_params[\"learning_rate\"]=1e-6\n",
    "base_params[\"experiment_name\"] = \"lr_1e-6_wd_0.42_broad\"\n",
    "\n",
    "# train the model\n",
    "\n",
    "# from models import train_model\n",
    "model = Classifier()\n",
    "\n",
    "model, train_losses, val_losses = train_model(model, federated_loader, validation_loader, num_epochs=100, lr=base_params[\"learning_rate\"],weight_decay=base_params[\"weight_decay\"])\n",
    "plot_losses([train_losses, val_losses], title=\"Losses on federated dataset\")\n",
    "\n",
    "save_model(model, params=base_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_params[\"weight_decay\"]=0.22\n",
    "base_params[\"learning_rate\"]=1e-4\n",
    "base_params[\"experiment_name\"] = \"lr_1e-4_wd_0.22_broad_25K\"\n",
    "\n",
    "# train the model\n",
    "model = Classifier()\n",
    "# from models import train_model\n",
    "\n",
    "model, train_losses, val_losses = train_model(model, federated_loader, validation_loader, num_epochs=20, lr=base_params[\"learning_rate\"],weight_decay=base_params[\"weight_decay\"])\n",
    "plot_losses([train_losses, val_losses], title=\"Losses on federated dataset\")\n",
    "\n",
    "save_model(model, params=base_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_params[\"weight_decay\"]=0.1\n",
    "base_params[\"learning_rate\"]=1e-5\n",
    "base_params[\"experiment_name\"] = \"lr_1e-5_wd_0.1_broad_25k\"\n",
    "# train the model\n",
    "model = Classifier()\n",
    "\n",
    "# from models import train_model\n",
    "\n",
    "model, train_losses, val_losses = train_model(model, federated_loader, validation_loader, num_epochs=50, lr=base_params[\"learning_rate\"],weight_decay=base_params[\"weight_decay\"])\n",
    "plot_losses([train_losses, val_losses], title=\"Losses on federated dataset\")\n",
    "\n",
    "save_model(model, params=base_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_params[\"weight_decay\"]=0.1\n",
    "base_params[\"learning_rate\"]=1e-5\n",
    "base_params[\"experiment_name\"] = \"lr_1e-1_wd_0.1_clean_only_25k\"\n",
    "# train the model\n",
    "model = Classifier()\n",
    "\n",
    "# from models import train_model\n",
    "\n",
    "model, train_losses, val_losses = train_model(model, training_loader, validation_loader, num_epochs=100, lr=base_params[\"learning_rate\"],weight_decay=base_params[\"weight_decay\"])\n",
    "plot_losses([train_losses, val_losses], title=\"Losses on training dataset\")\n",
    "\n",
    "save_model(model, params=base_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save model\n",
    "plot_losses([train_losses, val_losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses([train_losses, val_losses], logscale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_model(model, FT_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model, FT_params[\"checkpoint_dir\"], latest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter search with Ax - being done in seperate notebook\n",
    "print(\"model\")\n",
    "for idx, param in enumerate(model.parameters()):\n",
    "    print(idx, param.requires_grad, param.shape)\n",
    "\n",
    "print(\"hidden\")\n",
    "for idx, param in enumerate(model.hidden.parameters()):\n",
    "    print(idx, param.requires_grad, param.shape)\n",
    "\n",
    "print(\"batchnorm\")\n",
    "for idx, param in enumerate(model.batchnorm.parameters()):\n",
    "    print(idx, param.requires_grad, param.shape)\n",
    "\n",
    "print(\"output\")\n",
    "for idx, param in enumerate(model.output.parameters()):\n",
    "    print(idx, param.requires_grad, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in model:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no broad tuning\n",
    "\n",
    "# load model\n",
    "model = Classifier()\n",
    "# change params\n",
    "FT_params[\"experiment_name\"] = \"clean_only\"\n",
    "FT_params[\"checkpoint_dir\"] = os.path.join(FT_params[\"base_checkpoint_dir\"], FT_params[\"experiment_name\"])\n",
    "# make directory for checkpoints\n",
    "os.makedirs(FT_params[\"checkpoint_dir\"], exist_ok=True)\n",
    "\n",
    "\n",
    "FT_model, train_losses, val_losses = train_model(model, training_loader, validation_loader, num_epochs=FT_EPOCHS, lr=base_params[\"learning_rate\"])\n",
    "\n",
    "save_model(FT_model, params=FT_params)\n",
    "plot_losses([train_losses, val_losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no broad tuning change lr to 1e-6 \n",
    "\n",
    "# load model\n",
    "model = Classifier()\n",
    "# change params\n",
    "FT_params[\"experiment_name\"] = \"clean_only\"\n",
    "FT_params[\"checkpoint_dir\"] = os.path.join(FT_params[\"base_checkpoint_dir\"], FT_params[\"experiment_name\"])\n",
    "# make directory for checkpoints\n",
    "os.makedirs(FT_params[\"checkpoint_dir\"], exist_ok=True)\n",
    "\n",
    "\n",
    "FT_model, train_losses, val_losses = train_model(model, training_loader, validation_loader, num_epochs=FT_EPOCHS, lr=base_params[\"learning_rate\"])\n",
    "\n",
    "save_model(FT_model, params=FT_params)\n",
    "plot_losses([train_losses, val_losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning on clean dataset\n",
    "FT_params[\"experiment_name\"] = \"broad_tuning\"\n",
    "FT_params[\"checkpoint_dir\"] = os.path.join(FT_params[\"base_checkpoint_dir\"], FT_params[\"experiment_name\"])\n",
    "\n",
    "# load model\n",
    "model = load_model(model, FT_params[\"checkpoint_dir\"], latest=True)\n",
    "# change params\n",
    "FT_params[\"experiment_name\"] = \"FT_clean\"\n",
    "FT_params[\"checkpoint_dir\"] = os.path.join(FT_params[\"base_checkpoint_dir\"], FT_params[\"experiment_name\"])\n",
    "# make directory for checkpoints\n",
    "os.makedirs(FT_params[\"checkpoint_dir\"], exist_ok=True)\n",
    "\n",
    "\n",
    "FT_model, train_losses, val_losses = train_model(model, training_loader, validation_loader, num_epochs=FT_EPOCHS, lr=base_params[\"learning_rate\"])\n",
    "\n",
    "save_model(FT_model, params=FT_params)\n",
    "plot_losses([train_losses, val_losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the effect on batch sizes on training speed and accuracy\n",
    "times = []\n",
    "final_train_losses = []\n",
    "final_val_losses = []\n",
    "batch_sizes = []\n",
    "\n",
    "import time\n",
    "\n",
    "for index in range(10, 16):\n",
    "    \n",
    "    batch_size = 2**index\n",
    "    batch_sizes.append(batch_size)\n",
    "\n",
    "    print(batch_size)\n",
    "    # create empty model\n",
    "    model = Classifier()\n",
    "    # change params in datasets\n",
    "    N = batch_size\n",
    "\n",
    "\n",
    "    federated_loader = MyDataset(federated_data)\n",
    "    validation_loader = MyDataset(validation_data)\n",
    "    training_loader = MyDataset(clean_data)\n",
    "    testing_loader = MyDataset(target_data)\n",
    "\n",
    "\n",
    "    federated_loader = DataLoader(federated_loader, batch_size=N, shuffle=True)\n",
    "    validation_loader = DataLoader(validation_loader, batch_size=N, shuffle=True)\n",
    "    training_loader = DataLoader(training_loader, batch_size=N, shuffle=True)\n",
    "    testing_loader = DataLoader(testing_loader, batch_size=N, shuffle=True)\n",
    "\n",
    "    # start timer\n",
    "    start = time.time()\n",
    "\n",
    "    # train the model for 10 epochs\n",
    "\n",
    "    model, train_losses, val_losses = train_model(model, federated_loader, validation_loader, num_epochs=5, lr=base_params[\"learning_rate\"])\n",
    "    # end timer\n",
    "    end = time.time()\n",
    "\n",
    "    # append time to list\n",
    "    print(end-start)\n",
    "\n",
    "    plot_losses([train_losses, val_losses])\n",
    "\n",
    "    times.append(end-start)\n",
    "\n",
    "    # append final losses to list\n",
    "    final_train_losses.append(train_losses[-1])\n",
    "    final_val_losses.append(val_losses[-1])\n",
    "\n",
    "# plot the results batch size vs time\n",
    "plt.plot(batch_sizes, times)\n",
    "plt.xlabel(\"Batch size\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.title(\"Batch size vs time\")\n",
    "plt.show()\n",
    "\n",
    "# plot the results batch size vs loss\n",
    "plt.plot(batch_sizes, final_train_losses, label=\"Training loss\")\n",
    "plt.plot(batch_sizes, final_val_losses, label=\"Validation loss\")\n",
    "plt.xlabel(\"Batch size\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Batch size vs loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function to freeze the layers of a model up to a certain layer index\n",
    "def freeze_layers(model, layer_index):\n",
    "    \"\"\"\n",
    "    Takes in a model and a layer index, and freezes the hidden layers up to that index\n",
    "    \"\"\"\n",
    "    # accounts for layers and activations are in the same list as well as for 0 indexing\n",
    "    layer_index = ((layer_index +1)*2)-1\n",
    "    for i, param in enumerate(model.hidden.parameters()):\n",
    "        if i > layer_index:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    for i, param in enumerate(model.batchnorm.parameters()):\n",
    "        if i > layer_index:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    \n",
    "    return model            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment freezing layers\n",
    "\n",
    "FT_params[\"experiment_name\"] = \"broad_tuning\"\n",
    "FT_params[\"checkpoint_dir\"] = os.path.join(FT_params[\"base_checkpoint_dir\"], FT_params[\"experiment_name\"])\n",
    "\n",
    "# load model\n",
    "model = load_model(model, FT_params[\"checkpoint_dir\"], latest=True)\n",
    "\n",
    "FT_params[\"experiment_name\"] = \"FT_clean_freeze_0\"\n",
    "FT_params[\"freeze_layers\"] = 0\n",
    "FT_params[\"checkpoint_dir\"] = os.path.join(FT_params[\"base_checkpoint_dir\"], FT_params[\"experiment_name\"])\n",
    "# make directory for checkpoints\n",
    "os.makedirs(FT_params[\"checkpoint_dir\"], exist_ok=True)\n",
    "\n",
    "model = freeze_layers(model, FT_params[\"freeze_layers\"])\n",
    "\n",
    "FT_model, train_losses, val_losses = train_model(model, training_loader, validation_loader, num_epochs=FT_EPOCHS, lr=base_params[\"learning_rate\"])\n",
    "\n",
    "save_model(FT_model, params=FT_params)\n",
    "plot_losses([train_losses, val_losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FT_params[\"experiment_name\"] = \"broad_tuning\"\n",
    "FT_params[\"checkpoint_dir\"] = os.path.join(FT_params[\"base_checkpoint_dir\"], FT_params[\"experiment_name\"])\n",
    "\n",
    "# load model\n",
    "model = load_model(model, FT_params[\"checkpoint_dir\"], latest=True)\n",
    "\n",
    "FT_params[\"experiment_name\"] = \"FT_clean_freeze_1\"\n",
    "FT_params[\"freeze_layers\"] = 1\n",
    "FT_params[\"checkpoint_dir\"] = os.path.join(FT_params[\"base_checkpoint_dir\"], FT_params[\"experiment_name\"])\n",
    "# make directory for checkpoints\n",
    "os.makedirs(FT_params[\"checkpoint_dir\"], exist_ok=True)\n",
    "\n",
    "model = freeze_layers(model, FT_params[\"freeze_layers\"])\n",
    "\n",
    "FT_model, train_losses, val_losses = train_model(model, training_loader, validation_loader, num_epochs=FT_EPOCHS, lr=base_params[\"learning_rate\"])\n",
    "\n",
    "save_model(FT_model, params=FT_params)\n",
    "plot_losses([train_losses, val_losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FT_params[\"experiment_name\"] = \"broad_tuning\"\n",
    "FT_params[\"checkpoint_dir\"] = os.path.join(FT_params[\"base_checkpoint_dir\"], FT_params[\"experiment_name\"])\n",
    "\n",
    "# load model\n",
    "model = load_model(model, FT_params[\"checkpoint_dir\"], latest=True)\n",
    "\n",
    "FT_params[\"experiment_name\"] = \"FT_clean_freeze_2\"\n",
    "FT_params[\"freeze_layers\"] = 2\n",
    "FT_params[\"checkpoint_dir\"] = os.path.join(FT_params[\"base_checkpoint_dir\"], FT_params[\"experiment_name\"])\n",
    "# make directory for checkpoints\n",
    "os.makedirs(FT_params[\"checkpoint_dir\"], exist_ok=True)\n",
    "\n",
    "model = freeze_layers(model, FT_params[\"freeze_layers\"])\n",
    "\n",
    "FT_model, train_losses, val_losses = train_model(model, training_loader, validation_loader, num_epochs=FT_EPOCHS, lr=base_params[\"learning_rate\"])\n",
    "\n",
    "save_model(FT_model, params=FT_params)\n",
    "plot_losses([train_losses, val_losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FT_params[\"experiment_name\"] = \"broad_tuning\"\n",
    "FT_params[\"checkpoint_dir\"] = os.path.join(FT_params[\"base_checkpoint_dir\"], FT_params[\"experiment_name\"])\n",
    "\n",
    "# load model\n",
    "model = load_model(model, FT_params[\"checkpoint_dir\"], latest=True)\n",
    "\n",
    "FT_params[\"experiment_name\"] = \"FT_clean_freeze_3\"\n",
    "FT_params[\"freeze_layers\"] = 3\n",
    "FT_params[\"checkpoint_dir\"] = os.path.join(FT_params[\"base_checkpoint_dir\"], FT_params[\"experiment_name\"])\n",
    "# make directory for checkpoints\n",
    "os.makedirs(FT_params[\"checkpoint_dir\"], exist_ok=True)\n",
    "\n",
    "model = freeze_layers(model, FT_params[\"freeze_layers\"])\n",
    "\n",
    "FT_model, train_losses, val_losses = train_model(model, training_loader, validation_loader, num_epochs=FT_EPOCHS, lr=base_params[\"learning_rate\"])\n",
    "\n",
    "save_model(FT_model, params=FT_params)\n",
    "plot_losses([train_losses, val_losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses([train_losses, val_losses], logscale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradual fine tuning to come later\n",
    "import math\n",
    "number_of_gradual_steps = math.log2(math.floor(len(federated_data)/len(clean_data)))\n",
    "\n",
    "print(number_of_gradual_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDistGeom\n",
    "from rdkit.DataStructs.cDataStructs import TanimotoSimilarity\n",
    "#function that calculates the similarity score between two fingerprints\n",
    "def calculate_similarity(FP, target_FP, simi_type = 'Tanimoto'):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    \n",
    "    - FP ... fingerprint of input compound\n",
    "    - target_FP ... fingerprint of target compound\n",
    "    - type ... type of similarity score to be calculated\n",
    "    \n",
    "    Outputs:\n",
    "    - similarity_score ... similarity score between input and compound \n",
    "    \"\"\"\n",
    "    \n",
    "    if simi_type is \"Tanimoto\":\n",
    "        similarity_score = DataStructs.TanimotoSimilarity(FP, target_FP)\n",
    "        return similarity_score\n",
    "    else:\n",
    "        raise NotImplementedError(\"Only Tanimoto similarity is currently supported\")\n",
    "\n",
    "\n",
    "# function that calculates the average similarity score of a single compound to the Target set\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_target_similarity(FP, target_set, simi_type = 'Tanimoto', mean = None or \"mean\" or \"median\"):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    \n",
    "    - FP ... fingerprint of input compound - MUST BE THE SAME BETWEEN INPUTS\n",
    "    - target_set ... list of fingerprints of target compounds\n",
    "    - type ... type of similarity score to be calculated\n",
    "    \n",
    "    Outputs:\n",
    "    - similarity_score ... similarity score between input and compound \n",
    "    \"\"\"\n",
    "    \n",
    "    similarity_scores = np.array([])\n",
    "    \n",
    "    for target_FP in target_set.FP:\n",
    "        similarity_score = calculate_similarity(FP, target_FP, simi_type = simi_type)\n",
    "        np.append(similarity_scores, similarity_score)\n",
    "\n",
    "    if mean is not None:\n",
    "        if mean is \"mean\":\n",
    "            return np.mean(similarity_scores)\n",
    "        elif mean is \"median\":\n",
    "            return np.median(similarity_scores)\n",
    "        else:\n",
    "            raise NotImplementedError(\"mean type must be None, 'mean', or 'median'\")\n",
    "    else:\n",
    "        return similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def calculate_tanimoto_similarity(fp1, fp2):\n",
    "    # Convert fingerprints to numpy arrays\n",
    "    try:\n",
    "        # for pandas dataframes\n",
    "        arr1 = fp1.to_numpy()[0]\n",
    "        arr2 = fp2.to_numpy()[0]\n",
    "    except:\n",
    "        try:\n",
    "            # for lists\n",
    "            arr1 = np.asarray(fp1)\n",
    "            arr2 = np.asarray(fp2)\n",
    "        except:\n",
    "            raise ValueError(\"Input fingerprints must be pandas slices or lists/arrays\")\n",
    "\n",
    "    # Calculate dot product and norm of each fingerprint\n",
    "    dot_prod = dot(arr1, arr2)\n",
    "    norm1 = norm(arr1)\n",
    "    norm2 = norm(arr2)\n",
    "    \n",
    "    # Calculate Tanimoto similarity\n",
    "    similarity = dot_prod / (norm1**2 + norm2**2 - dot_prod)\n",
    "    return similarity\n",
    "\n",
    "# function that calculates the similarity score between two fingerprints\n",
    "def calculate_similarity(FP, target_FP, simi_type='Tanimoto'):\n",
    "    if simi_type == 'Tanimoto':\n",
    "        similarity_score = calculate_tanimoto_similarity(FP, target_FP)\n",
    "        return similarity_score\n",
    "    else:\n",
    "        raise NotImplementedError(\"Only Tanimoto similarity is currently supported\")\n",
    "\n",
    "# function that calculates the average similarity score of a single compound to the Target set\n",
    "def calculate_target_similarity(FP, target_set, simi_type='Tanimoto', mean=None):\n",
    "    similarity_scores = np.array([])\n",
    "\n",
    "    for target_FP in target_set.FP:\n",
    "        similarity_score = calculate_similarity(FP, target_FP, simi_type=simi_type)\n",
    "        np.append(similarity_scores, similarity_score)\n",
    "\n",
    "    if mean is not None:\n",
    "        if mean == 'mean':\n",
    "            return np.mean(similarity_scores)\n",
    "        elif mean == 'median':\n",
    "            return np.median(similarity_scores)\n",
    "        else:\n",
    "            raise NotImplementedError(\"mean type must be None, 'mean', or 'median'\")\n",
    "    else:\n",
    "        return similarity_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define a function that compiles the datasets based on specifications that we set for various combinations of domains (F, S0, T)\n",
    "def dataset_compiler(F_dataset=None, S0_dataset=None, target_dataset=None, percentages=None, rank = None or 'Tanimoto', random_state=42):\n",
    "    \"\"\"\n",
    "    Compiles the datasets into a single dataset that can then be loaded into the model\n",
    "\n",
    "    Inputs: \n",
    "    - F_dataset ... Federated dataset\n",
    "    - S0_dataset ... Source dataset \n",
    "    - target_dataset ... Target dataset\n",
    "\n",
    "    Parameters:\n",
    "    - percentages ... list of percentages for the federated and source datasets\n",
    "    - rank ... Choose rankings for the datasets when sampling\n",
    "\n",
    "    Outputs:\n",
    "    - dataset ... compiled dataset as a pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # check that the datasets are present\n",
    "    datasets = []\n",
    "    for dataset in [F_dataset, S0_dataset, target_dataset]:\n",
    "        if dataset is not None:\n",
    "            datasets.append(dataset)\n",
    "\n",
    "    if all(x is None for x in [F_dataset, S0_dataset, target_dataset]):\n",
    "        raise ValueError(\"No datasets have been specified\")\n",
    "    \n",
    "    # check that the percentages are present if not set to 100%\n",
    "    if percentages is None:\n",
    "        percentages = [1]*len(datasets)\n",
    "\n",
    "    if len(datasets) != len(percentages):\n",
    "        raise ValueError(\"The number of datasets must match the number of percentages\")\n",
    "        \n",
    "\n",
    "    # rank based on the specified ranking\n",
    "    if rank is not None:\n",
    "        #check if rank is in the dataset\n",
    "        for dataset in datasets:\n",
    "            if rank not in dataset.columns and target_dataset is not None:\n",
    "                print(\"Rank {} not in dataset, calculating ranks\".format(rank))\n",
    "                try:\n",
    "                    # calculate the ranks\n",
    "                    dataset[rank] = calculate_target_similarity(dataset, target_dataset, rank, mean=\"mean\")\n",
    "                except:\n",
    "                    raise ValueError(\"The rank is not in the dataset and cannot be calculated\")\n",
    "\n",
    "                # how does this work if the rank is not present in the dataset? for example in an empty dataset\n",
    "                dataset.sort_values(by=rank, ascending=False, inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "    # sample the datasets based on the percentages\n",
    "    for dataset, percentage in zip(datasets, percentages):\n",
    "        print(\"Initial size of dataset: {}\".format(len(dataset)))\n",
    "        print(\"Sampling {}% of the dataset\".format(percentage*100))\n",
    "        dataset = dataset.sample(frac=percentage, random_state=random_state)\n",
    "        print(\"Final size of dataset: {}\".format(len(dataset)))\n",
    "\n",
    "\n",
    "    # combine the datasets\n",
    "    compiled_dataset = pd.concat(datasets, axis=0)\n",
    "\n",
    "    return compiled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradual fine tuning testing - each finetuning step is 1/2 the size of the previous step for the federated dataset\n",
    "# Each step we compile a dataset with the federated data and the clean data and then train the model on that dataset\n",
    "import copy\n",
    "# load in the broad tuned model\n",
    "model = Classifier()\n",
    "#create placeholder for grad_FT_model\n",
    "Grad_FT_model = None\n",
    "\n",
    "# broad tuning on federated dataset\n",
    "base_params[\"experiment_name\"] = \"broad_tuning\"\n",
    "base_params[\"checkpoint_dir\"] = os.path.join(base_params[\"base_checkpoint_dir\"], base_params[\"experiment_name\"])\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 0.0\n",
    "\n",
    "# model = load_model(model, base_params[\"checkpoint_dir\"], latest=True)\n",
    "model, train_losses, val_losses = train_model(model, federated_loader, validation_loader, num_epochs=20, lr=learning_rate, weight_decay=weight_decay)\n",
    "plot_losses([train_losses, val_losses], title=\"Losses on compiled dataset\")\n",
    "\n",
    "\n",
    "\n",
    "# set the parameters for the gradual fine tuning\n",
    "gradual_FT_params = copy.deepcopy(base_params)\n",
    "gradual_FT_params[\"experiment_name\"] = \"gradual_FT\"\n",
    "gradual_FT_params[\"checkpoint_dir\"] = os.path.join(gradual_FT_params[\"base_checkpoint_dir\"], gradual_FT_params[\"experiment_name\"])\n",
    "\n",
    "\n",
    "\n",
    "gradual_FT_params[\"max_epochs\"] = 10\n",
    "batch_size = 2**10\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 0.0\n",
    "# make directory for checkpoints\n",
    "os.makedirs(gradual_FT_params[\"checkpoint_dir\"], exist_ok=True)\n",
    "\n",
    "number_of_gradual_steps = int(math.log2(math.floor(len(federated_data)/len(clean_data))))\n",
    "\n",
    "for i in range(number_of_gradual_steps):\n",
    "    temp_params = copy.deepcopy(gradual_FT_params)\n",
    "    if Grad_FT_model is not None:\n",
    "        model = Grad_FT_model\n",
    "    i = i+1\n",
    "    # calcualate percentages for the datasets\n",
    "    federated_percentage = 1/(2**i)\n",
    "    clean_percentage = 1\n",
    "    percentages = [federated_percentage, clean_percentage]\n",
    "    # compile the dataset\n",
    "    compiled_dataset = dataset_compiler(F_dataset=federated_data, S0_dataset=clean_data, percentages=percentages)\n",
    "    compiled_loader = MyDataset(compiled_dataset)\n",
    "    compiled_loader = DataLoader(compiled_loader, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    \n",
    "    # train the model using the compiled dataset\n",
    "    temp_params[\"experiment_name\"] = gradual_FT_params[\"experiment_name\"] + \"_step_{}\".format(i)\n",
    "    Grad_FT_model, train_losses, val_losses = train_model(model, compiled_loader, validation_loader, num_epochs=gradual_FT_params[\"max_epochs\"], lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    plot_losses([train_losses, val_losses], title=\"Losses on compiled dataset\"+\" \"+temp_params[\"experiment_name\"])\n",
    "\n",
    "    save_model(model, params=temp_params)\n",
    "\n",
    "\n",
    "# final step with the clean data\n",
    "temp_params[\"experiment_name\"] = gradual_FT_params[\"experiment_name\"] + \"_step_{}\".format(-1)\n",
    "\n",
    "Grad_FT_model, train_losses, val_losses = train_model(model, training_loader, validation_loader, num_epochs=gradual_FT_params[\"max_epochs\"], lr=1e-3)\n",
    "\n",
    "plot_losses([train_losses, val_losses], title=\"Losses on compiled dataset\"+\" \"+temp_params[\"experiment_name\"])\n",
    "save_model(model, params=temp_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradual fine tuning testing - each finetuning step is 1/2 the size of the previous step for the federated dataset\n",
    "# Each step we compile a dataset with the federated data and the clean data and then train the model on that dataset\n",
    "import copy\n",
    "# load in the broad tuned model\n",
    "model = Classifier()\n",
    "#create placeholder for grad_FT_model\n",
    "Grad_FT_model = None\n",
    "\n",
    "# broad tuning on federated dataset\n",
    "base_params[\"experiment_name\"] = \"broad_tuning\"\n",
    "base_params[\"checkpoint_dir\"] = os.path.join(base_params[\"base_checkpoint_dir\"], base_params[\"experiment_name\"])\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 0.1\n",
    "\n",
    "# model = load_model(model, base_params[\"checkpoint_dir\"], latest=True)\n",
    "model, train_losses, val_losses = train_model(model, federated_loader, validation_loader, num_epochs=20, lr=learning_rate, weight_decay=weight_decay)\n",
    "plot_losses([train_losses, val_losses], title=\"Losses on compiled dataset\")\n",
    "\n",
    "\n",
    "\n",
    "# set the parameters for the gradual fine tuning\n",
    "gradual_FT_params = copy.deepcopy(base_params)\n",
    "gradual_FT_params[\"experiment_name\"] = \"gradual_FT\"\n",
    "gradual_FT_params[\"checkpoint_dir\"] = os.path.join(gradual_FT_params[\"base_checkpoint_dir\"], gradual_FT_params[\"experiment_name\"])\n",
    "\n",
    "\n",
    "\n",
    "gradual_FT_params[\"max_epochs\"] = 10\n",
    "batch_size = 2**10\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 0.0\n",
    "# make directory for checkpoints\n",
    "os.makedirs(gradual_FT_params[\"checkpoint_dir\"], exist_ok=True)\n",
    "\n",
    "number_of_gradual_steps = int(math.log2(math.floor(len(federated_data)/len(clean_data))))\n",
    "\n",
    "for i in range(number_of_gradual_steps):\n",
    "    temp_params = copy.deepcopy(gradual_FT_params)\n",
    "    if Grad_FT_model is not None:\n",
    "        model = Grad_FT_model\n",
    "    i = i+1\n",
    "    # calcualate percentages for the datasets\n",
    "    federated_percentage = 1/(2**i)\n",
    "    clean_percentage = 1\n",
    "    percentages = [federated_percentage, clean_percentage]\n",
    "    # compile the dataset\n",
    "    compiled_dataset = dataset_compiler(F_dataset=federated_data, S0_dataset=clean_data, percentages=percentages)\n",
    "    compiled_loader = MyDataset(compiled_dataset)\n",
    "    compiled_loader = DataLoader(compiled_loader, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    \n",
    "    # train the model using the compiled dataset\n",
    "    temp_params[\"experiment_name\"] = gradual_FT_params[\"experiment_name\"] + \"_step_{}\".format(i)\n",
    "    Grad_FT_model, train_losses, val_losses = train_model(model, compiled_loader, validation_loader, num_epochs=gradual_FT_params[\"max_epochs\"], lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    plot_losses([train_losses, val_losses], title=\"Losses on compiled dataset\"+\" \"+temp_params[\"experiment_name\"])\n",
    "\n",
    "    save_model(model, params=temp_params)\n",
    "\n",
    "\n",
    "# final step with the clean data\n",
    "temp_params[\"experiment_name\"] = gradual_FT_params[\"experiment_name\"] + \"_step_{}\".format(-1)\n",
    "\n",
    "Grad_FT_model, train_losses, val_losses = train_model(model, training_loader, validation_loader, num_epochs=gradual_FT_params[\"max_epochs\"], lr=1e-3)\n",
    "\n",
    "plot_losses([train_losses, val_losses], title=\"Losses on compiled dataset\"+\" \"+temp_params[\"experiment_name\"])\n",
    "save_model(model, params=temp_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradual fine tuning testing - each finetuning step is 1/2 the size of the previous step for the federated dataset\n",
    "# Each step we compile a dataset with the federated data and the clean data and then train the model on that dataset\n",
    "import copy\n",
    "# load in the broad tuned model\n",
    "model = Classifier()\n",
    "#create placeholder for grad_FT_model\n",
    "Grad_FT_model = None\n",
    "\n",
    "# broad tuning on federated dataset\n",
    "base_params[\"experiment_name\"] = \"broad_tuning\"\n",
    "base_params[\"checkpoint_dir\"] = os.path.join(base_params[\"base_checkpoint_dir\"], base_params[\"experiment_name\"])\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 0.1\n",
    "\n",
    "# model = load_model(model, base_params[\"checkpoint_dir\"], latest=True)\n",
    "model, train_losses, val_losses = train_model(model, federated_loader, validation_loader, num_epochs=20, lr=learning_rate, weight_decay=weight_decay)\n",
    "plot_losses([train_losses, val_losses], title=\"Losses on compiled dataset\")\n",
    "\n",
    "\n",
    "\n",
    "# set the parameters for the gradual fine tuning\n",
    "gradual_FT_params = copy.deepcopy(base_params)\n",
    "gradual_FT_params[\"experiment_name\"] = \"gradual_FT\"\n",
    "gradual_FT_params[\"checkpoint_dir\"] = os.path.join(gradual_FT_params[\"base_checkpoint_dir\"], gradual_FT_params[\"experiment_name\"])\n",
    "\n",
    "\n",
    "\n",
    "gradual_FT_params[\"max_epochs\"] = 10\n",
    "batch_size = 2**10\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 0.1\n",
    "# make directory for checkpoints\n",
    "os.makedirs(gradual_FT_params[\"checkpoint_dir\"], exist_ok=True)\n",
    "\n",
    "number_of_gradual_steps = int(math.log2(math.floor(len(federated_data)/len(clean_data))))\n",
    "\n",
    "for i in range(number_of_gradual_steps):\n",
    "    temp_params = copy.deepcopy(gradual_FT_params)\n",
    "    if Grad_FT_model is not None:\n",
    "        model = Grad_FT_model\n",
    "    i = i+1\n",
    "    # calcualate percentages for the datasets\n",
    "    federated_percentage = 1/(2**i)\n",
    "    clean_percentage = 1\n",
    "    percentages = [federated_percentage, clean_percentage]\n",
    "    # compile the dataset\n",
    "    compiled_dataset = dataset_compiler(F_dataset=federated_data, S0_dataset=clean_data, percentages=percentages)\n",
    "    compiled_loader = MyDataset(compiled_dataset)\n",
    "    compiled_loader = DataLoader(compiled_loader, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    \n",
    "    # train the model using the compiled dataset\n",
    "    temp_params[\"experiment_name\"] = gradual_FT_params[\"experiment_name\"] + \"_step_{}\".format(i)\n",
    "    Grad_FT_model, train_losses, val_losses = train_model(model, compiled_loader, validation_loader, num_epochs=gradual_FT_params[\"max_epochs\"], lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    plot_losses([train_losses, val_losses], title=\"Losses on compiled dataset\"+\" \"+temp_params[\"experiment_name\"])\n",
    "\n",
    "    save_model(model, params=temp_params)\n",
    "\n",
    "\n",
    "# final step with the clean data\n",
    "temp_params[\"experiment_name\"] = gradual_FT_params[\"experiment_name\"] + \"_step_{}\".format(-1)\n",
    "\n",
    "Grad_FT_model, train_losses, val_losses = train_model(model, training_loader, validation_loader, num_epochs=gradual_FT_params[\"max_epochs\"], lr=1e-3)\n",
    "\n",
    "plot_losses([train_losses, val_losses], title=\"Losses on compiled dataset\"+\" \"+temp_params[\"experiment_name\"])\n",
    "save_model(model, params=temp_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradual fine tuning testing - each finetuning step is 1/2 the size of the previous step for the federated dataset\n",
    "# Each step we compile a dataset with the federated data and the clean data and then train the model on that dataset\n",
    "import copy\n",
    "# load in the broad tuned model\n",
    "model = Classifier()\n",
    "#create placeholder for grad_FT_model\n",
    "Grad_FT_model = None\n",
    "\n",
    "# broad tuning on federated dataset\n",
    "base_params[\"experiment_name\"] = \"broad_tuning\"\n",
    "base_params[\"checkpoint_dir\"] = os.path.join(base_params[\"base_checkpoint_dir\"], base_params[\"experiment_name\"])\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 0.1\n",
    "\n",
    "# model = load_model(model, base_params[\"checkpoint_dir\"], latest=True)\n",
    "model, train_losses, val_losses = train_model(model, federated_loader, validation_loader, num_epochs=20, lr=learning_rate, weight_decay=weight_decay)\n",
    "plot_losses([train_losses, val_losses], title=\"Losses on compiled dataset\")\n",
    "\n",
    "\n",
    "\n",
    "# set the parameters for the gradual fine tuning\n",
    "gradual_FT_params = copy.deepcopy(base_params)\n",
    "gradual_FT_params[\"experiment_name\"] = \"gradual_FT\"\n",
    "gradual_FT_params[\"checkpoint_dir\"] = os.path.join(gradual_FT_params[\"base_checkpoint_dir\"], gradual_FT_params[\"experiment_name\"])\n",
    "\n",
    "\n",
    "\n",
    "gradual_FT_params[\"max_epochs\"] = 10\n",
    "batch_size = 2**10\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 0.1\n",
    "# make directory for checkpoints\n",
    "os.makedirs(gradual_FT_params[\"checkpoint_dir\"], exist_ok=True)\n",
    "\n",
    "number_of_gradual_steps = int(math.log2(math.floor(len(federated_data)/len(clean_data))))\n",
    "\n",
    "for i in range(number_of_gradual_steps):\n",
    "    temp_params = copy.deepcopy(gradual_FT_params)\n",
    "    if Grad_FT_model is not None:\n",
    "        model = Grad_FT_model\n",
    "    i = i+1\n",
    "    # calcualate percentages for the datasets\n",
    "    federated_percentage = 1/(2**i)\n",
    "    clean_percentage = 1\n",
    "    percentages = [federated_percentage, clean_percentage]\n",
    "    # compile the dataset\n",
    "    compiled_dataset = dataset_compiler(F_dataset=federated_data, S0_dataset=clean_data, percentages=percentages)\n",
    "    compiled_loader = MyDataset(compiled_dataset)\n",
    "    compiled_loader = DataLoader(compiled_loader, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    \n",
    "    # train the model using the compiled dataset\n",
    "    temp_params[\"experiment_name\"] = gradual_FT_params[\"experiment_name\"] + \"_step_{}\".format(i)\n",
    "    Grad_FT_model, train_losses, val_losses = train_model(model, compiled_loader, validation_loader, num_epochs=gradual_FT_params[\"max_epochs\"], lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    plot_losses([train_losses, val_losses], title=\"Losses on compiled dataset\"+\" \"+temp_params[\"experiment_name\"])\n",
    "\n",
    "    save_model(model, params=temp_params)\n",
    "\n",
    "\n",
    "# final step with the clean data\n",
    "temp_params[\"experiment_name\"] = gradual_FT_params[\"experiment_name\"] + \"_step_{}\".format(-1)\n",
    "\n",
    "Grad_FT_model, train_losses, val_losses = train_model(model, training_loader, validation_loader, num_epochs=gradual_FT_params[\"max_epochs\"], lr=1e-3)\n",
    "\n",
    "plot_losses([train_losses, val_losses], title=\"Losses on compiled dataset\"+\" \"+temp_params[\"experiment_name\"])\n",
    "save_model(model, params=temp_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradual fine tuning testing - each finetuning step is 1/2 the size of the previous step for the federated dataset\n",
    "# Each step we compile a dataset with the federated data and the clean data and then train the model on that dataset\n",
    "import copy\n",
    "# load in the broad tuned model\n",
    "model = Classifier()\n",
    "#create placeholder for grad_FT_model\n",
    "Grad_FT_model = None\n",
    "\n",
    "# broad tuning on federated dataset\n",
    "base_params[\"experiment_name\"] = \"broad_tuning\"\n",
    "base_params[\"checkpoint_dir\"] = os.path.join(base_params[\"base_checkpoint_dir\"], base_params[\"experiment_name\"])\n",
    "learning_rate = 1e-5\n",
    "weight_decay = 0.1\n",
    "\n",
    "# model = load_model(model, base_params[\"checkpoint_dir\"], latest=True)\n",
    "model, train_losses, val_losses = train_model(model, federated_loader, validation_loader, num_epochs=20, lr=learning_rate, weight_decay=weight_decay)\n",
    "plot_losses([train_losses, val_losses], title=\"Losses on compiled dataset\")\n",
    "\n",
    "\n",
    "\n",
    "# set the parameters for the gradual fine tuning\n",
    "gradual_FT_params = copy.deepcopy(base_params)\n",
    "gradual_FT_params[\"experiment_name\"] = \"gradual_FT\"\n",
    "gradual_FT_params[\"checkpoint_dir\"] = os.path.join(gradual_FT_params[\"base_checkpoint_dir\"], gradual_FT_params[\"experiment_name\"])\n",
    "\n",
    "\n",
    "\n",
    "gradual_FT_params[\"max_epochs\"] = 10\n",
    "batch_size = 2**10\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 0.1\n",
    "# make directory for checkpoints\n",
    "os.makedirs(gradual_FT_params[\"checkpoint_dir\"], exist_ok=True)\n",
    "\n",
    "number_of_gradual_steps = int(math.log2(math.floor(len(federated_data)/len(clean_data))))\n",
    "\n",
    "for i in range(number_of_gradual_steps):\n",
    "    temp_params = copy.deepcopy(gradual_FT_params)\n",
    "    if Grad_FT_model is not None:\n",
    "        model = Grad_FT_model\n",
    "    i = i+1\n",
    "    # calcualate percentages for the datasets\n",
    "    federated_percentage = 1/(2**i)\n",
    "    clean_percentage = 1\n",
    "    percentages = [federated_percentage, clean_percentage]\n",
    "    # compile the dataset\n",
    "    compiled_dataset = dataset_compiler(F_dataset=federated_data, S0_dataset=clean_data, percentages=percentages)\n",
    "    compiled_loader = MyDataset(compiled_dataset)\n",
    "    compiled_loader = DataLoader(compiled_loader, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    \n",
    "    # train the model using the compiled dataset\n",
    "    temp_params[\"experiment_name\"] = gradual_FT_params[\"experiment_name\"] + \"_step_{}\".format(i)\n",
    "    Grad_FT_model, train_losses, val_losses = train_model(model, compiled_loader, validation_loader, num_epochs=gradual_FT_params[\"max_epochs\"], lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    plot_losses([train_losses, val_losses], title=\"Losses on compiled dataset\"+\" \"+temp_params[\"experiment_name\"])\n",
    "\n",
    "    save_model(model, params=temp_params)\n",
    "\n",
    "\n",
    "# final step with the clean data\n",
    "temp_params[\"experiment_name\"] = gradual_FT_params[\"experiment_name\"] + \"_step_{}\".format(-1)\n",
    "\n",
    "Grad_FT_model, train_losses, val_losses = train_model(model, training_loader, validation_loader, num_epochs=gradual_FT_params[\"max_epochs\"], lr=1e-3)\n",
    "\n",
    "plot_losses([train_losses, val_losses], title=\"Losses on compiled dataset\"+\" \"+temp_params[\"experiment_name\"])\n",
    "save_model(model, params=temp_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic fine tuning with tanimoto similarity applied at each loss calculation\n",
    "# first we are going to rank based on validation similarity\n",
    "# calculate similarity ranks for the datasets\n",
    "rank = 'tanimoto_rank'\n",
    "\n",
    "# federated_data[rank] = federated_data[\"FP\"].apply(calculate_target_similarity,target_set=validation_data, mean=\"mean\")\n",
    "clean_data[rank] = clean_data[\"FP\"].apply(calculate_target_similarity, target_set=validation_data, mean=\"mean\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(clean_data[rank]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = federated_data[\"FP\"].to_numpy()[0]\n",
    "\n",
    "calculate_tanimoto_similarity(FP, FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create dataset loaders\n",
    "\n",
    "federated_loader = MyDataset(federated_data, rank=rank)\n",
    "training_loader = MyDataset(clean_data, rank=rank)\n",
    "\n",
    "federated_loader = DataLoader(federated_loader, batch_size=25000, shuffle=True)\n",
    "training_loader = DataLoader(training_loader, batch_size=2**10, shuffle=True)\n",
    "\n",
    "\n",
    "# load in the broad tuned model\n",
    "model = Classifier()\n",
    "base_params[\"experiment_name\"] = \"broad_tuning\"\n",
    "base_params[\"checkpoint_dir\"] = os.path.join(base_params[\"base_checkpoint_dir\"], base_params[\"experiment_name\"])\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 0.1\n",
    "\n",
    "model = load_model(model, base_params[\"checkpoint_dir\"], latest=True)\n",
    "#retrain on the federated data\n",
    "model, train_losses, val_losses = train_model(model, federated_loader, validation_loader, num_epochs=20, lr=learning_rate, weight_decay=weight_decay)\n",
    "plot_losses([train_losses, val_losses], title=\"Losses on compiled dataset\")\n",
    "\n",
    "#try again from scratch\n",
    "model = Classifier()\n",
    "model, train_losses, val_losses = train_model(model, federated_loader, validation_loader, num_epochs=20, lr=learning_rate, weight_decay=weight_decay)\n",
    "plot_losses([train_losses, val_losses], title=\"Losses on compiled dataset\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fluid_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e243eb943314de6382befea42381295e09c15e9bf932d709b959fb3931a0818"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
